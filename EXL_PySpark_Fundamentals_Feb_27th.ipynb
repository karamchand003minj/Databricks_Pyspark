{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33446f8f-747f-4818-8311-7f8c18f84f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f0929e19f10>\n"
     ]
    }
   ],
   "source": [
    "# Creating SparkSession Object\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session_object = SparkSession.builder.getOrCreate()\n",
    "# spark_session_object = sparkSession.builder.appName(\"Exl PySpark Application\").getOrCreate()\n",
    "print(spark_session_object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68aaaf4e-2c05-483d-985b-4faeb7d0f359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n+----------+-----------+---------+--------+------+------+\n\nroot\n |-- First_Name: string (nullable = true)\n |-- Middle_Name: string (nullable = true)\n |-- Last_Name: string (nullable = true)\n |-- Emp_ID: integer (nullable = false)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Concept of Defining our own custom Datatypes & Nullable Constrains using - StructType & StructField\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "data_emp = [(\"Alex\",None,\"Tom\",3235245,\"M\",12000),\\\n",
    "          (\"John\",\"\",\"XXXX\",89983243,\"M\",12000), \\\n",
    "          (\"Andy\",\"Steve\",\"Morision\",435435,\"M\",12000), \\\n",
    "          (\"Arun\",\"OOOO\",\"Gopal\",454425,\"M\",12000), \\\n",
    "          (\"Claudia\",\"\",\"Schepars\",978643,\"F\",8000), \\\n",
    "          (\"Nancy\",\"MMMM\",\"Paul\",2353462,\"F\",12000)]\n",
    "\n",
    "schema_emp = StructType([StructField(\"First_Name\",StringType(),True),\\\n",
    "    StructField(\"Middle_Name\",StringType(),True),\\\n",
    "        StructField(\"Last_Name\",StringType(),True),\\\n",
    "            StructField(\"Emp_ID\",IntegerType(),False),\\\n",
    "                StructField(\"Gender\",StringType(),True),\\\n",
    "                    StructField(\"Salary\",IntegerType(),True),])\n",
    "\n",
    "df_emp_wit_cols_dt_nullable = spark_session_object.createDataFrame(data_emp,schema_emp)\n",
    "df_emp_wit_cols_dt_nullable.show()\n",
    "df_emp_wit_cols_dt_nullable.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9940f47f-3086-4295-b677-91d018228303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+----------+-----------+---------+--------+------+------+--------------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|gender_col_new|\n+----------+-----------+---------+--------+------+------+--------------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|          Male|\n|      John|           |     XXXX|89983243|     M| 12000|          Male|\n|      Andy|      Steve| Morision|  435435|     M| 12000|          Male|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|          Male|\n|   Claudia|           | Schepars|  978643|     F|  8000|        Female|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|        Female|\n+----------+-----------+---------+--------+------+------+--------------+\n\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|gender_col_new|yearly_bonus|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|          Male|      3600.0|\n|      John|           |     XXXX|89983243|     M| 12000|          Male|      3600.0|\n|      Andy|      Steve| Morision|  435435|     M| 12000|          Male|      3600.0|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|          Male|      3600.0|\n|   Claudia|           | Schepars|  978643|     F|  8000|        Female|      2400.0|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|        Female|      3600.0|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|gender_col_new|yearly_bonus|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|          Male|      3000.0|\n|      John|           |     XXXX|89983243|     M| 12000|          Male|      3000.0|\n|      Andy|      Steve| Morision|  435435|     M| 12000|          Male|      3000.0|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|          Male|      3000.0|\n|   Claudia|           | Schepars|  978643|     F|  8000|        Female|      2000.0|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|        Female|      3000.0|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|gender_col_new|yearly_bonus|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|          Male|      2400.0|\n|      John|           |     XXXX|89983243|     M| 12000|          Male|      2400.0|\n|      Andy|      Steve| Morision|  435435|     M| 12000|          Male|      2400.0|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|          Male|      2400.0|\n|   Claudia|           | Schepars|  978643|     F|  8000|        Female|      1600.0|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|        Female|      2400.0|\n+----------+-----------+---------+--------+------+------+--------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SparkSQL\n",
    "\n",
    "df_emp_wit_cols_dt_nullable.createOrReplaceTempView(\"tbl_employee_details\")\n",
    "spark_sql_emp_detials1 = spark_session_object.sql(\"select * from tbl_employee_details where Gender='M'\")\n",
    "spark_sql_emp_detials1.show()\n",
    "\n",
    "spark_sql_emp_detials2 = spark_session_object.sql(\"select *,(case when gender='M' then 'Male' when gender='F' then 'Female' else gender end) as gender_col_new from tbl_employee_details\")\n",
    "\n",
    "spark_sql_emp_detials2.show()\n",
    "\n",
    "# We are seeing 3 different ways of mentioning the columns inside the dataframe.\n",
    "\n",
    "# df_pyspark_trans_bonus = df_emp_wit_cols_dt_nullable.withColumn(\"yearly_bonus\",df_emp_wit_cols_dt_nullable.Salary*30/100)\n",
    "df_pyspark_trans_bonus = spark_sql_emp_detials2.withColumn(\"yearly_bonus\",df_emp_wit_cols_dt_nullable.Salary*30/100)\n",
    "\n",
    "df_pyspark_trans_bonus.show()\n",
    "\n",
    "# df_pyspark_trans_bonus_1 = df_emp_wit_cols_dt_nullable.withColumn(\"yearly_bonus\",df_emp_wit_cols_dt_nullable[\"Salary\"]*25/100)\n",
    "df_pyspark_trans_bonus_1 = spark_sql_emp_detials2.withColumn(\"yearly_bonus\",df_emp_wit_cols_dt_nullable[\"Salary\"]*25/100)\n",
    "\n",
    "df_pyspark_trans_bonus_1.show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# df_pyspark_trans_bonus_2 = df_emp_wit_cols_dt_nullable.withColumn(\"yearly_bonus\",col(\"Salary\")*20/100)\n",
    "df_pyspark_trans_bonus_2 = spark_sql_emp_detials2.withColumn(\"yearly_bonus\",col(\"Salary\")*20/100)\n",
    "\n",
    "df_pyspark_trans_bonus_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e3813c-dfa9-4777-ae97-81a42153656e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+--------------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Salary|gender_col_new|\n+----------+-----------+---------+--------+------+--------------+\n|      Alex|       null|      Tom| 3235245| 12000|          Male|\n|      John|           |     XXXX|89983243| 12000|          Male|\n|      Andy|      Steve| Morision|  435435| 12000|          Male|\n|      Arun|       OOOO|    Gopal|  454425| 12000|          Male|\n|   Claudia|           | Schepars|  978643|  8000|        Female|\n|     Nancy|       MMMM|     Paul| 2353462| 12000|        Female|\n+----------+-----------+---------+--------+------+--------------+\n\n+----------+-----------+---------+--------+------+------+-----------------+-----------------------+------+\n|First_Name|Middle_Name|Last_Name|Emp_ID  |Salary|Gender|Organization     |Comments               |Org ID|\n+----------+-----------+---------+--------+------+------+-----------------+-----------------------+------+\n|Alex      |null       |Tom      |3235245 |12000 |Male  |Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n|John      |           |XXXX     |89983243|12000 |Male  |Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n|Andy      |Steve      |Morision |435435  |12000 |Male  |Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n|Arun      |OOOO       |Gopal    |454425  |12000 |Male  |Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n|Claudia   |           |Schepars |978643  |8000  |Female|Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n|Nancy     |MMMM       |Paul     |2353462 |12000 |Female|Microsoft Pvt Ltd|Random values - shgdjds|50520 |\n+----------+-----------+---------+--------+------+------+-----------------+-----------------------+------+\n\n+----------+-----------+---------+--------+------+------+-----------------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Salary|Gender|     Organization|Org ID|\n+----------+-----------+---------+--------+------+------+-----------------+------+\n|      Alex|       null|      Tom| 3235245| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      John|           |     XXXX|89983243| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      Andy|      Steve| Morision|  435435| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      Arun|       OOOO|    Gopal|  454425| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|   Claudia|           | Schepars|  978643|  8000|Female|Microsoft Pvt Ltd| 50520|\n|     Nancy|       MMMM|     Paul| 2353462| 12000|Female|Microsoft Pvt Ltd| 50520|\n+----------+-----------+---------+--------+------+------+-----------------+------+\n\n+----------+-----------+---------+--------+------+------+-----------------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Salary|Gender|     Organization|Org ID|\n+----------+-----------+---------+--------+------+------+-----------------+------+\n|      Alex|       null|      Tom| 3235245| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      John|           |     XXXX|89983243| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      Andy|      Steve| Morision|  435435| 12000|  Male|Microsoft Pvt Ltd| 50520|\n|      Arun|       OOOO|    Gopal|  454425| 12000|  Male|Microsoft Pvt Ltd| 50520|\n+----------+-----------+---------+--------+------+------+-----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "df_pyspark_trans_3 = spark_sql_emp_detials2.drop('gender')\n",
    "df_pyspark_trans_3.show()\n",
    "\n",
    "df_pyspark_trans_4 = df_pyspark_trans_3.withColumnRenamed('gender_col_new','Gender').withColumn(\"Organization\",lit(\"Microsoft Pvt Ltd\"))\\\n",
    "    .withColumn(\"Comments\",lit(\"Random values - shgdjds\")).withColumn(\"Org ID\",lit(\"50520\"))\n",
    "# df_pyspark_trans_4.show()\n",
    "df_pyspark_trans_4.show(truncate=False)\n",
    "\n",
    "df_pyspark_trans_5 = df_pyspark_trans_4.drop('Comments')\n",
    "df_pyspark_trans_5.show()\n",
    "\n",
    "df_pyspark_trans_6 = df_pyspark_trans_5.filter(df_pyspark_trans_5['Gender'] == 'Male')\n",
    "df_pyspark_trans_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01887c6-4803-4d5f-8515-34813e66f798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|     karam|      chand|     minj| 2353462|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|     karam|      chand|     minj| 2353462|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|     karam|      chand|     minj| 2353462|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|     karam|      chand|     minj| 2353462|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|     karam|      chand|     minj| 2353462|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n+----------+-----------+---------+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Union - Merging data vertically, Duplicates concept\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "data_emp_one = [(\"Alex\",None,\"Tom\",3235245,\"M\",12000),\\\n",
    "          (\"John\",\"\",\"XXXX\",89983243,\"M\",12000), \\\n",
    "          (\"Andy\",\"Steve\",\"Morision\",435435,\"M\",12000), \\\n",
    "          (\"Arun\",\"OOOO\",\"Gopal\",454425,\"M\",12000), \\\n",
    "          (\"Claudia\",\"\",\"Schepars\",978643,\"F\",8000), \\\n",
    "          (\"Nancy\",\"MMMM\",\"Paul\",2353462,\"F\",12000)]\n",
    "\n",
    "data_emp_two = [(\"Alex\",None,\"Tom\",3235245,\"M\",12000),\\\n",
    "          (\"Arun\",\"OOOO\",\"Gopal\",454425,\"M\",12000), \\\n",
    "          (\"karam\",\"chand\",\"minj\",2353462,\"M\",12000)]\n",
    "\n",
    "schema_emp = StructType([StructField(\"First_Name\",StringType(),True),\\\n",
    "    StructField(\"Middle_Name\",StringType(),True),\\\n",
    "        StructField(\"Last_Name\",StringType(),True),\\\n",
    "            StructField(\"Emp_ID\",IntegerType(),False),\\\n",
    "                StructField(\"Gender\",StringType(),True),\\\n",
    "                    StructField(\"Salary\",IntegerType(),True),])\n",
    "\n",
    "df_pyspark_emp_one = spark_session_object.createDataFrame(data_emp_one,schema_emp)\n",
    "\n",
    "df_pyspark_emp_two = spark_session_object.createDataFrame(data_emp_two,schema_emp)\n",
    "\n",
    "# df_pyspark_emp_one.show()\n",
    "# df_pyspark_emp_two.show()\n",
    "\n",
    "df_pyspark_emp_union = df_pyspark_emp_one.union(df_pyspark_emp_two)\n",
    "df_pyspark_emp_union.show()\n",
    "\n",
    "df_pyspark_emp_dedup = df_pyspark_emp_union.dropDuplicates()\n",
    "df_pyspark_emp_dedup.show()\n",
    "\n",
    "df_pyspark_emp_dedup_another = df_pyspark_emp_union.distinct()\n",
    "df_pyspark_emp_dedup_another.show()\n",
    "\n",
    "# Sorting\n",
    "\n",
    "df_emp_sort = df_pyspark_emp_dedup_another.sort(col('Salary'))\n",
    "df_emp_sort.show()\n",
    "\n",
    "df_emp_sort_asc = df_pyspark_emp_dedup_another.sort(col('Salary'),ascending=False)\n",
    "df_emp_sort_asc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cc6b5f-8e7d-4608-8561-105bc1179701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|     karam|      chand|     minj| 2353462|     M| 12000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n+----------+-----------+---------+--------+------+------+\n\n+------+-----+\n|Gender|count|\n+------+-----+\n|     F|    2|\n|     M|    5|\n+------+-----+\n\n+----------+-----------+---------+------+-------+-------+\n|First_Name|Middle_Name|Last_Name|Salary|Country|Dept_ID|\n+----------+-----------+---------+------+-------+-------+\n|      Alex|       null|      Tom| 12000|  India|   D145|\n|      John|        ken|     XXXX|  9000|    USA|   D155|\n|      Andy|       DDDD|     YYYY|  7000|  India|   D145|\n|      Arun|           |     ZZZZ|  5000|    USA|   D165|\n|      John|        ken|     XXXX|  9000|    USA|   D155|\n|     Nancy|       MMMM|     LLLL| 12000|    GER|   D145|\n|       Sam|       HHHH|     KKKK| 60000|  India|   D175|\n+----------+-----------+---------+------+-------+-------+\n\n+---------+-------+\n|Dept_Name|Dept_ID|\n+---------+-------+\n|  Finance|   D145|\n|   Travel|   D155|\n|       IT|   D165|\n|Marketing|   D185|\n+---------+-------+\n\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|First_Name|Middle_Name|Last_Name|Salary|Country|Dept_ID|Dept_Name|Dept_ID|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|      Alex|       null|      Tom| 12000|  India|   D145|  Finance|   D145|\n|      Andy|       DDDD|     YYYY|  7000|  India|   D145|  Finance|   D145|\n|     Nancy|       MMMM|     LLLL| 12000|    GER|   D145|  Finance|   D145|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      Arun|           |     ZZZZ|  5000|    USA|   D165|       IT|   D165|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|First_Name|Middle_Name|Last_Name|Salary|Country|Dept_ID|Dept_Name|Dept_ID|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|      Alex|       null|      Tom| 12000|  India|   D145|  Finance|   D145|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      Andy|       DDDD|     YYYY|  7000|  India|   D145|  Finance|   D145|\n|      Arun|           |     ZZZZ|  5000|    USA|   D165|       IT|   D165|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|     Nancy|       MMMM|     LLLL| 12000|    GER|   D145|  Finance|   D145|\n|       Sam|       HHHH|     KKKK| 60000|  India|   D175|     null|   null|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|First_Name|Middle_Name|Last_Name|Salary|Country|Dept_ID|Dept_Name|Dept_ID|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|     Nancy|       MMMM|     LLLL| 12000|    GER|   D145|  Finance|   D145|\n|      Andy|       DDDD|     YYYY|  7000|  India|   D145|  Finance|   D145|\n|      Alex|       null|      Tom| 12000|  India|   D145|  Finance|   D145|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      Arun|           |     ZZZZ|  5000|    USA|   D165|       IT|   D165|\n|      null|       null|     null|  null|   null|   null|Marketing|   D185|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|First_Name|Middle_Name|Last_Name|Salary|Country|Dept_ID|Dept_Name|Dept_ID|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n|      Alex|       null|      Tom| 12000|  India|   D145|  Finance|   D145|\n|      Andy|       DDDD|     YYYY|  7000|  India|   D145|  Finance|   D145|\n|     Nancy|       MMMM|     LLLL| 12000|    GER|   D145|  Finance|   D145|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      John|        ken|     XXXX|  9000|    USA|   D155|   Travel|   D155|\n|      Arun|           |     ZZZZ|  5000|    USA|   D165|       IT|   D165|\n|       Sam|       HHHH|     KKKK| 60000|  India|   D175|     null|   null|\n|      null|       null|     null|  null|   null|   null|Marketing|   D185|\n+----------+-----------+---------+------+-------+-------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_name_sort = df_pyspark_emp_dedup_another.orderBy(col('First_Name').desc())\n",
    "df_pyspark_name_sort.show()\n",
    "\n",
    "df_pyspark_emp_dedup_another.count()\n",
    "\n",
    "df_pyspark_emp_dedup_another.groupBy(col('Gender')).count().show()\n",
    "\n",
    "# Another way fo importing it\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data_emp = [(\"Alex\",None,\"Tom\",12000,\"India\",\"D145\"), \\\n",
    "            (\"John\",\"ken\",\"XXXX\",9000,\"USA\",\"D155\"), \\\n",
    "            (\"Andy\",\"DDDD\",\"YYYY\",7000,\"India\",\"D145\"), \\\n",
    "            (\"Arun\",\"\",\"ZZZZ\",5000,\"USA\",\"D165\"), \\\n",
    "            (\"John\",\"ken\",\"XXXX\",9000,\"USA\",\"D155\"), \\\n",
    "            (\"Nancy\",\"MMMM\",\"LLLL\",12000,\"GER\",\"D145\"), \\\n",
    "            (\"Sam\",\"HHHH\",\"KKKK\",60000,\"India\",\"D175\")]\n",
    "\n",
    "schema_with_dt_emp = T.StructType([T.StructField(\"First_Name\",T.StringType(),True), \\\n",
    "                                        T.StructField(\"Middle_Name\",T.StringType(),True), \\\n",
    "                                        T.StructField(\"Last_Name\",T.StringType(),True), \\\n",
    "                                        T.StructField(\"Salary\",T.IntegerType(),True), \\\n",
    "                                T.StructField(\"Country\",T.StringType(),True),\\\n",
    "                                T.StructField(\"Dept_ID\",T.StringType(),True)])\n",
    "\n",
    "data_dept = [(\"Finance\",\"D145\"),\\\n",
    "          (\"Travel\",\"D155\"),\\\n",
    "          (\"IT\",\"D165\"),\\\n",
    "          (\"Marketing\",\"D185\")]\n",
    "\n",
    "schema_with_dt_dept = T.StructType([T.StructField(\"Dept_Name\",T.StringType(),True),\\\n",
    "                             T.StructField(\"Dept_ID\",T.StringType(),True)])\n",
    "\n",
    "df_pyspark_emp_join = spark_session_object.createDataFrame(data_emp,schema_with_dt_emp)\n",
    "df_pyspark_emp_join.show()\n",
    "\n",
    "df_pyspark_dept_join = spark_session_object.createDataFrame(data_dept,schema_with_dt_dept)\n",
    "df_pyspark_dept_join.show()\n",
    "\n",
    "df_pyspark_emp_dept_inner_joins = df_pyspark_emp_join.join(df_pyspark_dept_join,df_pyspark_emp_join.Dept_ID==df_pyspark_dept_join.Dept_ID,'inner')\n",
    "df_pyspark_emp_dept_inner_joins.show()\n",
    "\n",
    "df_pyspark_emp_dept_left_joins = df_pyspark_emp_join.join(df_pyspark_dept_join,df_pyspark_emp_join.Dept_ID==df_pyspark_dept_join.Dept_ID,'left')\n",
    "df_pyspark_emp_dept_left_joins.show()\n",
    "\n",
    "df_pyspark_emp_dept_right_joins = df_pyspark_emp_join.join(df_pyspark_dept_join,df_pyspark_emp_join.Dept_ID==df_pyspark_dept_join.Dept_ID,'right')\n",
    "df_pyspark_emp_dept_right_joins.show()\n",
    "\n",
    "df_pyspark_emp_dept_outer_joins = df_pyspark_emp_join.join(df_pyspark_dept_join,df_pyspark_emp_join.Dept_ID==df_pyspark_dept_join.Dept_ID,'outer')\n",
    "df_pyspark_emp_dept_outer_joins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b8ef8e-27e6-4d37-9f3e-c9e417d683d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EXL_PySpark_Fundamentals_Feb_27th",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
