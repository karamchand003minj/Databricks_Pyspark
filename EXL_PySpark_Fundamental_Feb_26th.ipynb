{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54340974-40d5-4d6e-989b-ace8b43037cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f1f6a457550>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91bd9849-3b76-4527-9cb7-1458c81e6510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Entry Points\n",
    "\"\"\"\n",
    "In Spark 1.x, There were 3 entry points\n",
    "SparkContext -> sc, it is an entry point for Spark. It is also used to created RDD(Resilient Distributed Datasets)\n",
    "SQLContext -> Using this you can write a SparkSQL Code. You can connect to different systems.\n",
    "HiveContext -> It is used to work with Hive QL.(HQL)\n",
    "\n",
    "From Spark 2.x -> A new entry point was introduced - SparkSession (A concept of DAtaFrame was introducted)\n",
    "During the Introductoin of SparkSession, They have combined the functionalites of SparkContext, SQLContext, HiveContext.\n",
    "\n",
    "RDD's and dataframe are immutable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b0848a-cd7b-4d17-be1d-d2479244d68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f362383cf10>\n"
     ]
    }
   ],
   "source": [
    "# Creating SparkSession Object\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session_object = SparkSession.builder.getOrCreate()\n",
    "# spark_session_object = sparkSession.builder.appName(\"Exl PySpark Application\").getOrCreate()\n",
    "print(spark_session_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34d87e6-ec92-4d61-bafa-1adfffbdfba0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n<class 'pyspark.rdd.RDD'>\nOut[3]: [47264846, 383726647, 'dkhguh475ud', 'fjhaekdhf384j']"
     ]
    }
   ],
   "source": [
    "list_of_values = [47264846,383726647,\"dkhguh475ud\",\"fjhaekdhf384j\"]\n",
    "\n",
    "print(type(list_of_values))\n",
    "\n",
    "rdd_random_values = spark_session_object.sparkContext.parallelize(list_of_values)\n",
    "print(type(rdd_random_values))\n",
    "\n",
    "rdd_random_values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7dc9d4-e061-4e1e-9997-31568fdf23a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n4\n1\n"
     ]
    }
   ],
   "source": [
    "print(rdd_random_values.getNumPartitions())\n",
    "\n",
    "rdd_random_values_2 = rdd_random_values.repartition(4) # You can increase or dercrease the partitions. (All partition shuffling will happending -> Impact in performance)\n",
    "print(rdd_random_values_2.getNumPartitions())\n",
    "\n",
    "rdd_random_values_3 = rdd_random_values.coalesce(1) # It will help you to only rduce the bumber fo partition. (only required data will be moved between partitions)\n",
    "print(rdd_random_values_3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66ff317-86f5-47a4-b1a5-4f4afe3d937c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: [(1, 'Databricks', '$1200'), (2, 'AWS', '$1900'), (3, 'Azure', '$1450')]"
     ]
    }
   ],
   "source": [
    "cloud_service_cost = [(1,\"Databricks\",\"$1200\"),(2,\"AWS\",\"$1900\"),(3,\"Azure\",\"$1450\")]\n",
    "\n",
    "rdd_cloud_cost = spark_session_object.sparkContext.parallelize(cloud_service_cost,3)\n",
    "\n",
    "rdd_cloud_cost.getNumPartitions()\n",
    "\n",
    "rdd_cloud_cost.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f0bf4a-a88d-41a1-bd53-e7dc13aa2028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_1: bigint, _2: string, _3: string]\n+---+----------+-----+\n| _1|        _2|   _3|\n+---+----------+-----+\n|  1|Databricks|$1200|\n|  2|       AWS|$1900|\n|  3|     Azure|$1450|\n+---+----------+-----+\n\n+---+----------+-----+\n| _1|        _2|   _3|\n+---+----------+-----+\n|  1|Databricks|$1200|\n|  2|       AWS|$1900|\n|  3|     Azure|$1450|\n+---+----------+-----+\n\n+----------+----------------------+-----+\n|Row_Number|Cloud_Service_Provider| Cost|\n+----------+----------------------+-----+\n|         1|            Databricks|$1200|\n|         2|                   AWS|$1900|\n|         3|                 Azure|$1450|\n+----------+----------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_from_rdd_cloud_cost = rdd_cloud_cost.toDF()\n",
    "print(df_from_rdd_cloud_cost)\n",
    "\n",
    "df_from_rdd_cloud_cost.show()\n",
    "\n",
    "df_from_rdd_cloud_cost_another = spark_session_object.createDataFrame(rdd_cloud_cost)\n",
    "df_from_rdd_cloud_cost_another.show()\n",
    "\n",
    "cloud_cost_col_names = [\"Row_Number\",\"Cloud_Service_Provider\",\"Cost\"]\n",
    "df_from_rdd_cloud_cost_with_cols = rdd_cloud_cost.toDF(cloud_cost_col_names)\n",
    "df_from_rdd_cloud_cost_with_cols.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81c2a68-8274-4ec4-81e6-94b1f03e2a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n+-------+-----+--------+--------+---+-----+\n|     _1|   _2|      _3|      _4| _5|   _6|\n+-------+-----+--------+--------+---+-----+\n|   Alex| null|     Tom| 3235245|  M|12000|\n|   John|     |    XXXX|89983243|  M|12000|\n|   Andy|Steve|Morision|  435435|  M|12000|\n|   Arun| OOOO|   Gopal|  454425|  M|12000|\n|Claudia|     |Schepars|  978643|  F| 8000|\n|  Nancy| MMMM|    Paul| 2353462|  F|12000|\n+-------+-----+--------+--------+---+-----+\n\n+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n+----------+-----------+---------+--------+------+------+\n\nroot\n |-- First_Name: string (nullable = true)\n |-- Middle_Name: string (nullable = true)\n |-- Last_Name: string (nullable = true)\n |-- Emp_ID: long (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data_emp = [(\"Alex\",None,\"Tom\",3235245,\"M\",12000),\\\n",
    "          (\"John\",\"\",\"XXXX\",89983243,\"M\",12000), \\\n",
    "          (\"Andy\",\"Steve\",\"Morision\",435435,\"M\",12000), \\\n",
    "          (\"Arun\",\"OOOO\",\"Gopal\",454425,\"M\",12000), \\\n",
    "          (\"Claudia\",\"\",\"Schepars\",978643,\"F\",8000), \\\n",
    "          (\"Nancy\",\"MMMM\",\"Paul\",2353462,\"F\",12000)]\n",
    "\n",
    "df_emp = spark_session_object.createDataFrame(data_emp)\n",
    "print(type(df_emp))\n",
    "\n",
    "df_emp.show()\n",
    "\n",
    "cols_emp = [\"First_Name\",\"Middle_Name\",\"Last_Name\",\"Emp_ID\",\"Gender\",\"Salary\"]\n",
    "\n",
    "df_emp_wit_cols = spark_session_object.createDataFrame(data_emp,cols_emp)\n",
    "df_emp_wit_cols.show()\n",
    "\n",
    "df_emp_wit_cols.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da8c693-1ed9-4552-9428-4e81094a1d05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+------+------+\n|First_Name|Middle_Name|Last_Name|  Emp_ID|Gender|Salary|\n+----------+-----------+---------+--------+------+------+\n|      Alex|       null|      Tom| 3235245|     M| 12000|\n|      John|           |     XXXX|89983243|     M| 12000|\n|      Andy|      Steve| Morision|  435435|     M| 12000|\n|      Arun|       OOOO|    Gopal|  454425|     M| 12000|\n|   Claudia|           | Schepars|  978643|     F|  8000|\n|     Nancy|       MMMM|     Paul| 2353462|     F| 12000|\n+----------+-----------+---------+--------+------+------+\n\nroot\n |-- First_Name: string (nullable = true)\n |-- Middle_Name: string (nullable = true)\n |-- Last_Name: string (nullable = true)\n |-- Emp_ID: integer (nullable = false)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Concept of Defining our own custom Datatypes & Nullable Constrains using - StructType & StructField\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "data_emp = [(\"Alex\",None,\"Tom\",3235245,\"M\",12000),\\\n",
    "          (\"John\",\"\",\"XXXX\",89983243,\"M\",12000), \\\n",
    "          (\"Andy\",\"Steve\",\"Morision\",435435,\"M\",12000), \\\n",
    "          (\"Arun\",\"OOOO\",\"Gopal\",454425,\"M\",12000), \\\n",
    "          (\"Claudia\",\"\",\"Schepars\",978643,\"F\",8000), \\\n",
    "          (\"Nancy\",\"MMMM\",\"Paul\",2353462,\"F\",12000)]\n",
    "\n",
    "schema_emp = StructType([StructField(\"First_Name\",StringType(),True),\\\n",
    "    StructField(\"Middle_Name\",StringType(),True),\\\n",
    "        StructField(\"Last_Name\",StringType(),True),\\\n",
    "            StructField(\"Emp_ID\",IntegerType(),False),\\\n",
    "                StructField(\"Gender\",StringType(),True),\\\n",
    "                    StructField(\"Salary\",IntegerType(),True),])\n",
    "\n",
    "df_emp_wit_cols_dt_nullable = spark_session_object.createDataFrame(data_emp,schema_emp)\n",
    "df_emp_wit_cols_dt_nullable.show()\n",
    "df_emp_wit_cols_dt_nullable.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09026681-b08e-4b27-915a-dfa0cec0d09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+------+\n|           Full_Name| Emp_ID|Gender|Salary|\n+--------------------+-------+------+------+\n|   {Alex, null, Tom}|3235245|     M| 12000|\n|      {John, , XXXX}| 883243|     M|  8000|\n|{Andy, Steve, Mor...| 435435|     M|  9000|\n| {Arun, OOOO, Gopal}| 454425|     M| 11500|\n|{Claudia, , Schep...| 978643|     F|  7000|\n| {Nancy, MMMM, Paul}|2353462|     F| 12000|\n+--------------------+-------+------+------+\n\nroot\n |-- Full_Name: struct (nullable = true)\n |    |-- First_Name: string (nullable = true)\n |    |-- Middle_Name: string (nullable = true)\n |    |-- Last_Name: string (nullable = true)\n |-- Emp_ID: integer (nullable = false)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Nested data\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "nested_data_emp = [((\"Alex\",None,\"Tom\"),3235245,\"M\",12000),\\\n",
    "          ((\"John\",\"\",\"XXXX\"),883243,\"M\",8000), \\\n",
    "          ((\"Andy\",\"Steve\",\"Morision\"),435435,\"M\",9000), \\\n",
    "          ((\"Arun\",\"OOOO\",\"Gopal\"),454425,\"M\",11500), \\\n",
    "          ((\"Claudia\",\"\",\"Schepars\"),978643,\"F\",7000), \\\n",
    "          ((\"Nancy\",\"MMMM\",\"Paul\"),2353462,\"F\",12000)]\n",
    "\n",
    "schema_nested_emp = StructType([StructField(\"Full_Name\",StructType([StructField(\"First_Name\",StringType(),True),StructField(\"Middle_Name\",StringType(),True),StructField(\"Last_Name\",StringType(),True)])),\\\n",
    "    StructField(\"Emp_ID\",IntegerType(),False),\\\n",
    "        StructField(\"Gender\",StringType(),True),\\\n",
    "            StructField(\"Salary\",IntegerType(),True)])\n",
    "\n",
    "df_nested_emp_wit_cols_dt_nullable = spark_session_object.createDataFrame(nested_data_emp,schema_nested_emp)\n",
    "df_nested_emp_wit_cols_dt_nullable.show()\n",
    "df_nested_emp_wit_cols_dt_nullable.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EXL_PySpark_Fundamental_Feb_26th",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
